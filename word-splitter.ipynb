{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1fe76d-0bba-4f68-9f53-61a01718b620",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "Develop a machine learning model that takes a concatenated text string without spaces (e.g., `\"helloworld\"`) as input and outputs the correctly spaced version (e.g., `\"hello world\"`). The goal is to accurately predict word boundaries and insert spaces in the appropriate positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d5840-4821-4d9d-ab82-15d96906d459",
   "metadata": {},
   "source": [
    "# Let's create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d223ba-d1dc-4e2f-a5e1-f9371a55a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a746086-66a9-404c-bd6d-b65a865e7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_corpus = load_dataset(\"bookcorpus/bookcorpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0991662-dcd3-4c35-bcda-0cd459ea41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db1874-0564-4c1a-8291-cdfdbe735e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(example):\n",
    "    example['text_no_space'] = example['text'].replace(\" \", \"\")\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41c830-25b9-4516-b352-037406a87611",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = book_corpus.select(range(0, 1_000_000)).map(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43a986-a015-42d3-a393-871d662cdbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f5937-435d-4473-b392-21b3f76f7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data.save_to_disk(f\"data/processed_bookcorpus_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f20a79-3de8-4481-a3f2-33323fa7b84f",
   "metadata": {},
   "source": [
    "### Training on 1M rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2dce3a-0620-4085-b4eb-2e290e236afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5abf0d-9ced-4180-9f2e-2c7f08339cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = load_from_disk(\"data/processed_bookcorpus_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc603ba-77a2-4658-bd7b-79251ba67adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'usually , he would be tearing around the living room , playing with his toys .',\n",
       " 'text_no_space': 'usually,hewouldbetearingaroundthelivingroom,playingwithhistoys.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822f5e24-d917-404d-a781-0a6aea96e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    texts = ''.join(texts)\n",
    "    print(f'total chars {len(texts)}')\n",
    "    chars = set(''.join(texts))\n",
    "    vocab = {char: idx for idx, char in enumerate(chars, start=2)}  # Reserve 0 for padding, 1 for <unk>\n",
    "    vocab[\"<pad>\"] = 0\n",
    "    vocab[\"<unk>\"] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a338e738-216d-489d-a27c-77363a9ba566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars 53467665\n"
     ]
    }
   ],
   "source": [
    "texts = [example[\"text_no_space\"] for example in text_data] \n",
    "vocab = build_vocab(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b7f456-5419-4b1a-b07d-c6adc0550124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted(vocab.keys())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9c58508-2826-44bd-8cad-7c1b94269af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "\n",
    "n_vocab = len(vocab)\n",
    "emd_dim = 128\n",
    "hidden_size = 128\n",
    "num_lstm_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0f2902e-4b72-44d0-82cb-bf66a23bde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a0205a2-fa7b-434d-af76-3248117148bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSplitterDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"text\"]\n",
    "\n",
    "        n = len(text)\n",
    "\n",
    "        inp = []\n",
    "        out = []\n",
    "\n",
    "        for i in range(0, n):\n",
    "            if text[i] == ' ':\n",
    "                continue;\n",
    "            inp.append(self.vocab.get(text[i], self.vocab[\"<unk>\"]))\n",
    "            if i < n - 1 and text[i + 1] == ' ':\n",
    "                out.append(1)\n",
    "            else:\n",
    "                out.append(0)\n",
    "                \n",
    "        return torch.tensor(inp, dtype=torch.long), torch.tensor(out, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "166d31f0-2ec0-4d8f-9c15-ccc630cdc71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'text_no_space'],\n",
       "    num_rows: 800000\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data_splits[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d637dfaa-b290-4ce5-ac79-93fe6ccc267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = text_data.train_test_split(0.2)\n",
    "test_split = data_splits[\"test\"].train_test_split(0.5)\n",
    "\n",
    "train_data = data_splits[\"train\"].select(range(0, 2_000_00))\n",
    "val_data = test_split[\"train\"].select(range(0, 50_000))\n",
    "test_data = test_split[\"test\"].select(range(0, 50_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2fdf298-fa92-4d66-903b-9044e5b289f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'text_no_space'],\n",
       "     num_rows: 200000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'text_no_space'],\n",
       "     num_rows: 50000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'text_no_space'],\n",
       "     num_rows: 50000\n",
       " }))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24cccc76-55ac-4c05-8c52-607f841f3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch, max_length=100):\n",
    "    inputs, labels = zip(*batch)\n",
    "\n",
    "    truncated_inputs = [seq[:max_length] for seq in inputs]\n",
    "    truncated_labels = [seq[:max_length] for seq in labels]\n",
    "\n",
    "    padded_inputs = pad_sequence(truncated_inputs, batch_first=True, padding_value=0)\n",
    "    padded_labels = pad_sequence(truncated_labels, batch_first=True, padding_value=-100)  # Use -100 for ignored labels\n",
    "\n",
    "    return padded_inputs, padded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e062b4d3-670f-4961-9420-8749f724c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(WordSplitterDataset(train_data, vocab), batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader =  DataLoader(WordSplitterDataset(val_data, vocab), batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31b719ab-9e14-4ada-b7c1-c6d63015129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LSTMWordSplitterModel(nn.Module):\n",
    "    def __init__(self, n_vocab, emd_dim, hidden_size,\n",
    "                 num_lstm_layers):\n",
    "        super(LSTMWordSplitterModel, self).__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=emd_dim,\n",
    "        )\n",
    "\n",
    "        \n",
    "        # LSMT layer\n",
    "        self.lstm = nn.LSTM(input_size=emd_dim, hidden_size=hidden_size,\n",
    "                            num_layers=num_lstm_layers, dropout=0.1,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "\n",
    "        # FCN layer\n",
    "        linear_h_size = 2 * hidden_size # BiLSTM\n",
    "        \n",
    "        self.first_fcn = nn.Linear(linear_h_size, linear_h_size)\n",
    "        self.second_fcn = nn.Linear(linear_h_size, 2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)        \n",
    "        out, _ = self.lstm(embed)        \n",
    "        out = self.first_fcn(out)\n",
    "        out = self.relu(out)\n",
    "        logits = self.second_fcn(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a064352-4b52-4c9f-ac21-4c8121e97a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMWordSplitterModel(n_vocab=n_vocab,\n",
    "                              emd_dim=emd_dim,\n",
    "                              hidden_size=hidden_size,\n",
    "                              num_lstm_layers=num_lstm_layers\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70d1f0e2-0e96-47ac-8443-e995a38b308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b102786-4741-47d1-bf90-8a915dcb8ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1129602"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03ad5a8b-e882-4aac-85c7-217260965a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e74298a6-cb9b-41c3-9bae-bc3251e63dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad99f90b-22bc-4ade-8b62-ba3d95e0310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d3a481f7-d17c-459e-9869-11ed8ddcea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4dea60cb-d36e-4d69-8e2c-d2b21d22ea73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c11a4455ec4459812bed618a81f1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Training Loss: 0.3983\n",
      "Epoch 1, Batch 200, Training Loss: 0.2288\n",
      "Epoch 1, Batch 300, Training Loss: 0.1583\n",
      "Epoch 1, Batch 400, Training Loss: 0.1394\n",
      "Epoch 1, Batch 500, Training Loss: 0.1312\n",
      "Epoch 1, Batch 600, Training Loss: 0.1012\n",
      "Epoch 1, Batch 700, Training Loss: 0.0973\n",
      "Epoch 1, Batch 800, Training Loss: 0.0930\n",
      "Epoch 1, Batch 900, Training Loss: 0.0865\n",
      "Epoch 1, Batch 1000, Training Loss: 0.0908\n",
      "Epoch 1, Batch 1100, Training Loss: 0.0888\n",
      "Epoch 1, Batch 1200, Training Loss: 0.0686\n",
      "Epoch 1, Batch 1300, Training Loss: 0.0858\n",
      "Epoch 1, Batch 1400, Training Loss: 0.0852\n",
      "Epoch 1, Batch 1500, Training Loss: 0.0807\n",
      "Epoch 1, Batch 1600, Training Loss: 0.0815\n",
      "Epoch 1, Batch 1700, Training Loss: 0.0708\n",
      "Epoch 1, Batch 1800, Training Loss: 0.0657\n",
      "Epoch 1, Batch 1900, Training Loss: 0.0675\n",
      "Epoch 1, Batch 2000, Training Loss: 0.0746\n",
      "Epoch 1, Batch 2100, Training Loss: 0.0533\n",
      "Epoch 1, Batch 2200, Training Loss: 0.0603\n",
      "Epoch 1, Batch 2300, Training Loss: 0.0552\n",
      "Epoch 1, Batch 2400, Training Loss: 0.0566\n",
      "Epoch 1, Batch 2500, Training Loss: 0.0505\n",
      "Epoch 1, Batch 2600, Training Loss: 0.0589\n",
      "Epoch 1, Batch 2700, Training Loss: 0.0479\n",
      "Epoch 1, Batch 2800, Training Loss: 0.0484\n",
      "Epoch 1, Batch 2900, Training Loss: 0.0498\n",
      "Epoch 1, Batch 3000, Training Loss: 0.0566\n",
      "Epoch 1, Batch 3100, Training Loss: 0.0430\n",
      "Epoch 1, Average Training Loss: 0.1018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc04256fcad452c84c4b9c2d17603e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Validation Loss: 0.0440, Validation Accuracy: 0.9840\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")):\n",
    "        inputs = inputs.to(device)  \n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(inputs)\n",
    "\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        writer.add_scalar(\"Loss/Train Batch\", loss.item(), epoch * len(train_dataloader) + batch_idx)\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            tqdm.write(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}, Training Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    tqdm.write(f\"Epoch {epoch + 1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/Train Epoch\", avg_train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(val_dataloader, desc=f\"Validation Epoch {epoch + 1}\")):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "\n",
    "            val_loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            writer.add_scalar(\"Loss/Validation Batch\", val_loss.item(), epoch * len(val_dataloader) + batch_idx)\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            valid_mask = labels.view(-1) != -100  # Mask to ignore padding\n",
    "            valid_predictions = predictions.view(-1)[valid_mask]\n",
    "            valid_labels = labels.view(-1)[valid_mask]\n",
    "    \n",
    "            correct_predictions += (valid_predictions == valid_labels).sum().item()\n",
    "            total_predictions += valid_labels.numel()\n",
    "\n",
    "        \n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    tqdm.write(f\"Epoch {epoch + 1}, Average Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b4b73f0-7a47-4ef1-babb-359084d0b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, val_loss, filepath):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"val_loss\": val_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved at {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b4f66898-c963-49a6-98f6-ae68e4b96ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/model_epoch_1.pth\n"
     ]
    }
   ],
   "source": [
    "save_checkpoint(\n",
    "    1,\n",
    "    model,\n",
    "    optimizer,\n",
    "    val_loss=avg_val_loss,\n",
    "    filepath=os.path.join(checkpoint_dir, f\"model_epoch_{1}.pth\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1fff276f-f4f8-4bd6-a802-042e67f869e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, vocab, input_text, max_length=100, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    input_indices = [vocab.get(char, vocab[\"<unk>\"]) for char in input_text]\n",
    "    if len(input_indices) > max_length:\n",
    "        input_indices = input_indices[:max_length]\n",
    "    input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)  # Shape: (1, seq_len, num_classes)\n",
    "        predictions = logits.argmax(dim=-1).squeeze(0).tolist()  # Predicted labels\n",
    "\n",
    "    split_text = \"\"\n",
    "    for i, char in enumerate(input_text[:max_length]):\n",
    "        split_text += char\n",
    "        if predictions[i] == 1:\n",
    "            split_text += \" \"\n",
    "\n",
    "    return split_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bf0babc1-4f16-43a4-91c9-4e667bf33a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"himynameisx\",\n",
    "    \"thequickbrownfoxjumpsoverthelazydog\",\n",
    "    \"machinelearningisfun\",\n",
    "    \"iloveprogramming\",\n",
    "    \"deepneuralnetworksarepowerful\",\n",
    "    \"welcometopytorchtraining\",\n",
    "    \"hellohowareyou\",\n",
    "    \"thisisatestsentence\",\n",
    "    \"naturallanguageprocessing\",\n",
    "    \"How are you?\",\n",
    "    \"Let'ssee,yourgame\",\n",
    "    \"whatis1+1?\",\n",
    "    \"ihave4tasks\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fddab662-3e55-426b-84c6-ab113a18b3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "himynameisx                                        | himy name isx                                     \n",
      "thequickbrownfoxjumpsoverthelazydog                | the quick brown fox jumps over the lazy dog       \n",
      "machinelearningisfun                               | machine learning is fun                           \n",
      "iloveprogramming                                   | i love programming                                \n",
      "deepneuralnetworksarepowerful                      | deep neural net works a repowerful                \n",
      "welcometopytorchtraining                           | welcome to pytorchtraining                        \n",
      "hellohowareyou                                     | hello how are you                                 \n",
      "thisisatestsentence                                | this isa testsentence                             \n",
      "naturallanguageprocessing                          | natural language processing                       \n",
      "How are you?                                       | How  are  you ?                                   \n",
      "Let'ssee,yourgame                                  | Let 's see , your game                            \n",
      "whatis1+1?                                         | what is 1+1 ?                                     \n",
      "ihave4tasks                                        | i have 4 t asks                                   \n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "    predicted_text = predict(model, vocab, sentence, device=device)\n",
    "    \n",
    "    print(f\"{sentence:<50} | {predicted_text:<50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb8356-ab91-4f23-b824-c0886a2099e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
